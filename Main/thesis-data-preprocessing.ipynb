{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install bangla-stemmer","metadata":{"execution":{"iopub.status.busy":"2021-09-13T13:17:20.912082Z","iopub.execute_input":"2021-09-13T13:17:20.912400Z","iopub.status.idle":"2021-09-13T13:19:50.814376Z","shell.execute_reply.started":"2021-09-13T13:17:20.912318Z","shell.execute_reply":"2021-09-13T13:19:50.813024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# import tensorflow as tf\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom nltk.tokenize import WhitespaceTokenizer\n\nfrom bangla_stemmer.stemmer import stemmer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labelled_authentic = pd.read_csv(\"data/BanFakeNews/LabeledAuthentic-7K.csv\")\nlabelled_fake = pd.read_csv(\"data/BanFakeNews/LabeledFake-1K.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labelled_authentic.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labelled_fake.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labelled_authentic.drop(\"articleID\", axis = 1, inplace = True)\nlabelled_authentic.drop(\"domain\", axis = 1, inplace = True)\nlabelled_authentic.drop(\"date\", axis = 1, inplace = True)\nlabelled_authentic.drop(\"category\", axis = 1, inplace = True)\nlabelled_authentic.drop(\"source\", axis = 1, inplace = True)\nlabelled_authentic.drop(\"relation\", axis = 1, inplace = True)\nlabelled_authentic.drop(\"headline\", axis = 1, inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labelled_authentic.tail()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labelled_fake.drop(\"articleID\", axis = 1, inplace = True)\nlabelled_fake.drop(\"domain\", axis = 1, inplace = True)\nlabelled_fake.drop(\"date\", axis = 1, inplace = True)\nlabelled_fake.drop(\"category\", axis = 1, inplace = True)\nlabelled_fake.drop(\"source\", axis = 1, inplace = True)\nlabelled_fake.drop(\"relation\", axis = 1, inplace = True)\nlabelled_fake.drop(\"headline\", axis = 1, inplace = True)\nlabelled_fake.drop(\"F-type\", axis = 1, inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labelled_fake.tail()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labelled_authentic.to_csv(\"data/BanFakeNews/Updated/labelled_authentic.csv\", index = False)\nlabelled_fake.to_csv(\"data/BanFakeNews/Updated/labelled_fake.csv\", index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labelled_combined = pd.read_csv(\"data/BanFakeNews/Updated/labelled_combined.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labelled_combined.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labelled_combined.tail()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Function for standardizing text (removing punctuations and bangla digits)","metadata":{}},{"cell_type":"code","source":"def standardize_text(df, feature):\n    df[feature] = df[feature].str.replace(r\"[(),!?@\\'\\/\\`\\-\\\"\\_\\n]\", \" \")\n    df[feature] = df[feature].str.replace(r\"।\", \" \")\n    df[feature] = df[feature].str.replace(r\"[১২৩৪৫৬৭৮৯০]\", \" \")\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_standardized = standardize_text(labelled_combined, \"content\")\ndata_standardized.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_standardized.content[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Function for tokenizing text","metadata":{}},{"cell_type":"code","source":"def tokenizing(df, token_feature, feature):\n    tokenizer = WhitespaceTokenizer()\n    df[token_feature] = df[feature].apply(tokenizer.tokenize)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_tokenized = data_standardized.copy()\n\n# tokenizing(data_set_tokenized, \"headline_tokens\", \"headline\")\ntokenizing(data_tokenized, \"content_tokens\", \"content\")\n\ndata_tokenized.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing stop words","metadata":{}},{"cell_type":"code","source":"with open('data/BanFakeNews/Updated/stop_words.txt', 'r', encoding=\"utf8\") as f:\n    stop_words = [line.strip() for line in f]\n\nprint(stop_words)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_stop_removed = data_tokenized.copy()\n\nfor i in range(0, 8501):\n    for s in data_stop_removed[\"content_tokens\"][i]:\n        if s in stop_words:\n            data_stop_removed[\"content_tokens\"][i].remove(s)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_stop_removed[\"content_tokens\"][0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Function for stemming","metadata":{}},{"cell_type":"code","source":"def stemming(df, feature, start, end):\n    stmr = stemmer.BanglaStemmer()\n    \n    for i in range(start, end + 1):\n        stm = stmr.stem(df[feature][i])\n        df[feature][i] = stm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_stemmed = data_stop_removed.copy()\nstemming(data_stemmed, \"content_tokens\", 0, 8500)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_stemmed.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Function for turning list to string","metadata":{}},{"cell_type":"code","source":"def list_to_string(df, feature_1, feature_2, start, end):\n    \n    for i in range(start, end + 1):\n        list = df[feature_2][i]\n        df[feature_1][i] = ' '.join(list)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_final = data_stemmed.copy()\nlist_to_string(data_final, \"content\", \"content_tokens\", 0, 8500)\ndata_final.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_final.drop(\"content_tokens\", axis = 1, inplace = True)\ndata_final.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_final.to_csv(\"data/BanFakeNews/Updated/data_final.csv\", index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}